{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeswariesamsetti/FMML-LABS-/blob/main/FMML_M4L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyMhDmOed0RJ"
      },
      "source": [
        "# FOUNDATIONS OF MODERN MACHINE LEARNING, IIIT Hyderabad\n",
        "# Module 4: Perceptron and Gradient Descent\n",
        "## Lab 2: Introduction to Gradient Descent\n",
        "\n",
        "Gradient descent is a very important algorithm to understand, as it underpins many of the more advanced algorithms used in Machine Learning and Deep Learning.\n",
        "\n",
        "A brief overview of the algorithm is\n",
        "\n",
        "\n",
        "*   start with a random initialization of the solution.\n",
        "*   incrementally change the solution by moving in the direction of negative gradient of the objective function.\n",
        "*   repeat the previous step until some convergence criteria is met.\n",
        "\n",
        "The key equation for change in weight is:\n",
        "$$w^{k+1} \\leftarrow w^k - \\eta \\Delta J$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "frl0cf4hQOPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sFrL73maQRmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx5OzL5jbnkO"
      },
      "source": [
        "# Importing the required libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQpDHGOAh0It"
      },
      "source": [
        "We can start be choosing coefficients for a second degree polynomial equation $(a x^2 + bx + c)$ that will distribute the data we will try to model.\n",
        "\n",
        "Let's define some random x data (inputs) we hope to predict y (outputs) of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnbvlEbWcUtM"
      },
      "source": [
        "def eval_2nd_degree(coeffs, x):\n",
        "    \"\"\"\n",
        "    Function to return the output of evaluating a second degree polynomial,\n",
        "    given a specific x value.\n",
        "\n",
        "    Args:\n",
        "        coeffs: List containing the coefficients a, b, and c for the polynomial.\n",
        "        x: The input x value to the polynomial.\n",
        "\n",
        "    Returns:\n",
        "        y: The corresponding output y value for the second degree polynomial.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * x\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "    return y\n",
        "\n",
        "hundred_xs = np.random.uniform(-10, 10, 100)\n",
        "coeffs = [1, 0, 0]\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_2nd_degree(coeffs, x)\n",
        "    xs.append(x)\n",
        "    ys.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a-Tzv5fclE2"
      },
      "source": [
        "plt.plot(xs, ys, 'g+')\n",
        "plt.title('Original data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQr81EuciKhB"
      },
      "source": [
        "This is good, but we could improve on this by making things more realistic. You can add noise or **jitter** to the values so they can resemble real-world data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggni_nKPdFZ5"
      },
      "source": [
        "def eval_2nd_degree_jitter(coeffs, x, j):\n",
        "    \"\"\"\n",
        "    Function to return the noisy output of evaluating a second degree polynomial,\n",
        "    given a specific x value. Output values can be within [y − j, y + j].\n",
        "\n",
        "    Args:\n",
        "        coeffs: List containing the coefficients a, b, and c for the polynomial.\n",
        "        x: The input x value to the polynomial.\n",
        "        j: Jitter parameter, to introduce noise to output y.\n",
        "\n",
        "    Returns:\n",
        "        y: The corresponding jittered output y value for the second degree polynomial.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * x\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "\n",
        "    interval = [y - j, y + j]\n",
        "    interval_min = interval[0]\n",
        "    interval_max = interval[1]\n",
        "    jit_val = random.random() * interval_max      # Generate a random number in range 0 to interval max\n",
        "\n",
        "    while interval_min > jit_val:                 # While the random jitter value is less than the interval min,\n",
        "        jit_val = random.random() * interval_max  # it is not in the right range. Re-roll the generator until it\n",
        "                                                  # give a number greater than the interval min.\n",
        "\n",
        "    return jit_val\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_2nd_degree_jitter(coeffs, x, 0.1)\n",
        "    xs.append(x)\n",
        "    ys.append(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFYv43vpe5Y4"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+')\n",
        "plt.title('Original data with jitter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umByA5Ghi_gt"
      },
      "source": [
        "We will now build our predictive model, and optimize it with gradient descent and we will try to get as close to these values as possible.\n",
        "\n",
        "To get a quantifiable measure of how incorrect it is, we calculate the Mean Squared Error loss for the model. This is the mean value of the sum of the squared differences between the actual and predicted outputs.\n",
        "\n",
        "$$ E = \\frac{1}{n} \\sum_{i=0}^n (y_i - \\bar{y_i})^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGo9VtQDfG6F"
      },
      "source": [
        "def loss_mse(ys, y_bar):\n",
        "    \"\"\"\n",
        "    Calculates MSE loss.\n",
        "\n",
        "    Args:\n",
        "        ys: training data labels\n",
        "        y_bar: prediction labels\n",
        "\n",
        "    Returns: Calculated MSE loss.\n",
        "    \"\"\"\n",
        "\n",
        "    return sum((ys - y_bar) * (ys - y_bar)) / len(ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIRquRB3kcZA"
      },
      "source": [
        "rand_coeffs = (random.randrange(-10, 10), random.randrange(-10, 10), random.randrange(-10, 10))\n",
        "y_bar = eval_2nd_degree(rand_coeffs, hundred_xs)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, y_bar, 'ro', label = 'prediction')\n",
        "plt.title('Original data vs first prediction')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbwBb4Ckomw"
      },
      "source": [
        "initial_model_loss = loss_mse(ys, y_bar)\n",
        "initial_model_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEcvjxbJa8cq"
      },
      "source": [
        "We can see that the loss is quite a large number. Let’s now see if we can improve on this fairly high loss metric by optimizing the model with gradient descent.\n",
        "\n",
        "We wish to improve our model. Therefore we want to alter its coefficients $a$, $b$ and $c$ to decrease the error. Therefore we require knowledge about how each coefficient affects the error. This is achieved by calculating the partial derivative of the loss function with respect to **each** of the individual coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhiloANqkSFc"
      },
      "source": [
        "def calc_gradient_2nd_poly(rand_coeffs, hundred_xs, ys):\n",
        "    \"\"\"\n",
        "    calculates the gradient for a second degree polynomial.\n",
        "\n",
        "    Args:\n",
        "        coeffs: a,b and c, for a 2nd degree polynomial [ y = ax^2 + bx + c ]\n",
        "        inputs_x: x input datapoints\n",
        "        outputs_y: actual y output points\n",
        "\n",
        "    Returns: Calculated gradients for the 2nd degree polynomial, as a tuple of its parts for a,b,c respectively.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "\n",
        "    y_bars = eval_2nd_degree(rand_coeffs, hundred_xs)\n",
        "\n",
        "    for x, y, y_bar in list(zip(hundred_xs, ys, y_bars)):    # take tuple of (x datapoint, actual y label, predicted y label)\n",
        "        x_squared = x ** 2\n",
        "        partial_a = x_squared * (y - y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = x * (y - y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = (y - y_bar)\n",
        "        c_s.append(partial_c)\n",
        "\n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "\n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "\n",
        "    return (gradient_a, gradient_b, gradient_c)   # return calculated gradients as a a tuple of its 3 parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN0jR2Dhkpjn"
      },
      "source": [
        "calc_grad = calc_gradient_2nd_poly(rand_coeffs, hundred_xs, ys)\n",
        "\n",
        "lr = 0.0001\n",
        "a_new = rand_coeffs[0] - lr * calc_grad[0]\n",
        "b_new = rand_coeffs[1] - lr * calc_grad[1]\n",
        "c_new = rand_coeffs[2] - lr * calc_grad[2]\n",
        "\n",
        "new_model_coeffs = (a_new, b_new, c_new)\n",
        "print(f\"New model coeffs: {new_model_coeffs}\")\n",
        "\n",
        "# update with these new coeffs:\n",
        "new_y_bar = eval_2nd_degree(new_model_coeffs, hundred_xs)\n",
        "updated_model_loss = loss_mse(ys, new_y_bar)\n",
        "\n",
        "print(f\"Now have smaller model loss: {updated_model_loss} vs {initial_model_loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rjqrqclk4BI"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original model')\n",
        "plt.plot(xs, y_bar, 'ro', label = 'first prediction')\n",
        "plt.plot(xs, new_y_bar, 'b.', label = 'updated prediction')\n",
        "plt.title('Original model vs 1st prediction vs updated prediction with lower loss')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOzSlzJIfvid"
      },
      "source": [
        "We’re almost ready. The last step will be to perform gradient descent iteratively over a number of epochs (cycles or iterations.) With every epoch we hope to see an improvement in the form of lowered loss, and better model-fitting to the original data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBkU4dRnlHKy"
      },
      "source": [
        "def calc_gradient_2nd_poly_for_GD(coeffs, inputs_x, outputs_y, lr):\n",
        "    \"\"\"\n",
        "    calculates the gradient for a second degree polynomial.\n",
        "\n",
        "    Args:\n",
        "        coeffs: a,b and c, for a 2nd degree polynomial [ y = ax^2 + bx + c ]\n",
        "        inputs_x: x input datapoints\n",
        "        outputs_y: actual y output points\n",
        "        lr: learning rate\n",
        "\n",
        "    Returns: Calculated gradients for the 2nd degree polynomial, as a tuple of its parts for a,b,c respectively.\n",
        "\n",
        "    \"\"\"\n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "\n",
        "    y_bars = eval_2nd_degree(coeffs, inputs_x)\n",
        "\n",
        "    for x,y,y_bar in list(zip(inputs_x, outputs_y, y_bars)):    # take tuple of (x datapoint, actual y label, predicted y label)\n",
        "        x_squared = x ** 2\n",
        "        partial_a = x_squared * (y - y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = x * (y - y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = (y - y_bar)\n",
        "        c_s.append(partial_c)\n",
        "\n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "\n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "\n",
        "\n",
        "    a_new = coeffs[0] - lr * gradient_a\n",
        "    b_new = coeffs[1] - lr * gradient_b\n",
        "    c_new = coeffs[2] - lr * gradient_c\n",
        "\n",
        "    new_model_coeffs = (a_new, b_new, c_new)\n",
        "\n",
        "    # update with these new coeffs:\n",
        "    new_y_bar = eval_2nd_degree(new_model_coeffs, inputs_x)\n",
        "\n",
        "    updated_model_loss = loss_mse(outputs_y, new_y_bar)\n",
        "    return updated_model_loss, new_model_coeffs, new_y_bar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj6K6SXol_bi"
      },
      "source": [
        "def gradient_descent(epochs, lr):\n",
        "    \"\"\"\n",
        "    Perform gradient descent for a second degree polynomial.\n",
        "\n",
        "    Args:\n",
        "        epochs: number of iterations to perform of finding new coefficients and updatingt loss.\n",
        "        lr: specified learning rate\n",
        "\n",
        "    Returns: Tuple containing (updated_model_loss, new_model_coeffs, new_y_bar predictions, saved loss updates)\n",
        "\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    rand_coeffs_to_test = rand_coeffs\n",
        "    for i in range(epochs):\n",
        "        loss = calc_gradient_2nd_poly_for_GD(rand_coeffs_to_test, hundred_xs, ys, lr)\n",
        "        rand_coeffs_to_test = loss[1]\n",
        "        losses.append(loss[0])\n",
        "    print(losses)\n",
        "    return loss[0], loss[1], loss[2], losses  # (updated_model_loss, new_model_coeffs, new_y_bar, saved loss updates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GD = gradient_descent(30000, 0.0003)\n"
      ],
      "metadata": {
        "id": "z3lunb-DH0HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brk2qRFlmAQM"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, GD[2], 'b.', label = 'final_prediction')\n",
        "plt.title('Original vs Final prediction after Gradient Descent')\n",
        "plt.legend(loc = \"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS2KZ6SxfnAI"
      },
      "source": [
        "This trained model is showing vast improvements after it’s full training cycle. We can examine further by inspecting its final predicted coefficients $a$, $b$ and $c$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efY8ehhvmCRz"
      },
      "source": [
        "print(f\"Final Coefficients predicted: {GD[1]}\")\n",
        "print(f\"Original Coefficients: {coeffs}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8PuwB87fjP5"
      },
      "source": [
        "Not too far off! A big improvement over the initial random model. Looking at the plot of the loss reduction over training offers further insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnswAURtmFBG"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(GD[3], 'b-', label = 'loss')\n",
        "# plt.xlim(0,50)\n",
        "plt.title('Loss over 500 iterations')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlim((0,100))\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu7fnsphdJpo"
      },
      "source": [
        "We observe that the model loss reached close to zero, to give us our more accurate coefficients. We can also see there was no major improvement in loss after about 100 epochs. An alternative strategy would be to add some kind of condition to the training step that stops training when a certain minimum loss threshold has been reached. This would prevent excessive training and potential over-fitting for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3oxAVdtePYa"
      },
      "source": [
        "# Things to try\n",
        "\n",
        "\n",
        "\n",
        "1.   Change the coefficients array and try a different polynomial instead of our $x^2$.\n",
        "2.   Increase/decrease the learning rate to see how many iterations will be take to coverge. Does it even converge on a huge learning rate?\n",
        "3. Take a degree 5 polynomial with 5 roots and try different initializations, instead of random ones. Does it converge on different values for different initializations? Why does initialization not matter in our case of $x^2$?\n",
        "4. Can you modify the algorithm to find a maxima of a function, instead of a minima?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Additional Critical Thinking Questions**"
      ],
      "metadata": {
        "id": "0eGY6YVWc769"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. You are doing full batch gradient descent using the entire training set. Is it necessary to shuffle the training data? Explain your\n",
        "answer.\n",
        "Your friend's advice is correct: you absolutely need to shuffle the training set when using mini-batch gradient descent, especially when the data is ordered by class. Here’s why:\n",
        "\n",
        "Why Shuffling is Necessary\n",
        "\n",
        "1. Preventing Class Bias in Batches:\n",
        "\n",
        "If the training data is not shuffled and all dog images are processed before cat images, mini-batches will contain only one class at a time (e.g., all dogs or all cats). This creates class-specific gradient updates, which can lead to poor generalization and slow convergence because the model might oscillate between fitting one class and the other.\n",
        "\n",
        "Shuffling ensures that each mini-batch is more representative of the overall distribution of the training data, containing a mix of both classes.\n",
        "\n",
        "\n",
        "\n",
        "2. Breaking Sequential Patterns:\n",
        "\n",
        "Sequential patterns in the dataset (e.g., all dogs followed by all cats) can introduce biases in how the model learns features. Shuffling disrupts this order, encouraging the model to learn more generalizable patterns.\n",
        "\n",
        "\n",
        "\n",
        "3. Improved Convergence:\n",
        "\n",
        "Shuffled data helps the model converge faster by providing more diverse and balanced gradient updates. This reduces the risk of the model overfitting to patterns introduced by the ordering of the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "How to Shuffle the Data\n",
        "\n",
        "Before each training epoch, shuffle the training dataset to ensure that the mini-batches drawn during that epoch are randomly sampled. Most deep learning frameworks (e.g., TensorFlow, PyTorch) provide built-in methods to shuffle data during training.\n",
        "\n",
        "Summary\n",
        "\n",
        "Shuffling the training set before using mini-batch gradient descent is crucial for balanced and unbiased learning. Without shuffling, the model may not learn effectively, leading to slower convergence and poorer generalization.\n",
        "\n",
        "\n",
        "2. You would like to train a dog/cat image classifier using mini-batch gradient\n",
        "descent. You have already split your dataset into train, dev and test sets. The classes\n",
        "are balanced. You realize that within the training set, the images are ordered in such a\n",
        "way that all the dog images come first and all the cat images come after. A friend tells\n",
        "you: ”you absolutely need to shuffle your training set before the training procedure.”\n",
        "Is your friend right? Explain\n",
        "No, it is not necessary to shuffle the training data when using full-batch gradient descent. Here’s why:\n",
        "\n",
        "1. All data is used in every update: In full-batch gradient descent, the gradient is computed over the entire training dataset during each iteration. Since the entire dataset is used, the order of the data points does not affect the gradient computation or the model updates.\n",
        "\n",
        "\n",
        "2. No randomness involved: Shuffling the data only matters in stochastic or mini-batch gradient descent, where a subset of the data (a batch) is sampled in each iteration. For these methods, shuffling ensures that batches are representative and not biased by the order of data points.\n",
        "\n",
        "\n",
        "\n",
        "When shuffling matters\n",
        "\n",
        "Shuffling is critical for stochastic gradient descent (SGD) and mini-batch gradient descent, especially if the training data is ordered or grouped (e.g., by class). Without shuffling, the model might learn biased or non-generalizable patterns.\n",
        "\n",
        "In contrast, full-batch gradient descent inherently averages over all data points in every iteration, making shuffling unnecessary."
      ],
      "metadata": {
        "id": "Au-awSN2fEI8"
      }
    }
  ]
}